---
title: "Challenge B"
author: "Audrey Pujol et Yann Cecconi"
date: "30 novembre 2017"
output: html_document
---

https://github.com/AudreyPujol/ChallengeB-Yann-Cecconi-Audrey-Pujol

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Task 1B - Predicting house prices in Ames, Iowa


#Step 1

Random Forest is an ensemble learning  (both classification and regression) technique. It is one of the commonly used predictive modelling and machine learning technique.

Key advantages of using Random Forest

- Reduce chances of over-fitting
- Higher model performance or accuracy

#Step 2

In order to train random forest on the training data, we first deal with the missing values of the corresponding data frame.
```{r, include=FALSE}
library(readr)
train <- read.csv(file="~/rprog/train.csv")
test <- read.csv(file="~/rprog/test.csv")
library(dplyr)
library(ggplot2)
library(randomForest)
library(tidyr)
```


```{r, include= FALSE}
train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

train <- train %>% filter(is.na(GarageType) == FALSE, is.na(MasVnrType) == FALSE, is.na(BsmtFinType2) == FALSE, is.na(BsmtExposure) == FALSE, is.na(Electrical) == FALSE)


train %>% summarise_all(.funs = funs(sum(is.na(.)))) %>% gather(key = "feature", value = "missing.observations") %>% filter(missing.observations > 0)

```

Then, we test the randomForest code on the model that we created in Challenge A ( based on the training data) :

```{r}

randomForest ( SalePrice ~ LotArea + LotShape + LandContour + Condition2 + OverallQual
               + OverallCond + PoolArea + KitchenQual + LotConfig 
               + Neighborhood+RoofMatl 
          + MasVnrArea + ExterQual + BsmtQual +  BsmtExposure + BsmtFinType1 
          +  BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF + X1stFlrSF +  X2ndFlrSF, data=train)

```


#Step 3

First we make predictions on the test data :

```{r}

rf <- randomForest ( SalePrice ~ LotArea + LotShape + LandContour + Condition2 + OverallQual
               + OverallCond + PoolArea + KitchenQual + LotConfig 
               + Neighborhood+RoofMatl 
          + MasVnrArea + ExterQual + BsmtQual +  BsmtExposure + BsmtFinType1 
          +  BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF + X1stFlrSF +  X2ndFlrSF, data=train)

predict(rf, data= test, type="response")

```

Then we make predictions on the linear regression of our choice :

```{r}
model <- lm(SalePrice ~ LotArea + LotShape + LandContour + Condition2 + OverallQual
               + OverallCond + PoolArea + KitchenQual + LotConfig 
               + Neighborhood+RoofMatl 
          + MasVnrArea + ExterQual + BsmtQual +  BsmtExposure + BsmtFinType1 
          +  BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF + X1stFlrSF +  X2ndFlrSF, data=train)

predict(model, data=test, type="response")


```

Comments :

Predictions made by the randomForest command are more relevant and trustworthy than the ones created by the linear regression.

For the first house for instance, we see that there is a difference of approximately 1000$ between the selling prices predicted by the codes.




## Task 2B - Predicting house prices in Ames, Iowa


```{r, include= FALSE, results= FALSE}
library(tidyverse)
library(np)
library(caret)
```

```{r, include= FALSE}
set.seed(1)
Nsim <- 150
b <- c(0,1)
x0 <- rep(1, Nsim)
x1 <- rnorm(n = Nsim)

X <- cbind(x0, x1^3)
y.true <- X %*% b

eps <- rnorm(n = Nsim)
y <- X %*% b + eps

df <- tbl_df(y[,1]) %>% rename(y = value) %>% bind_cols(tbl_df(x1)) %>% rename(x = value) %>% bind_cols(tbl_df(y.true[,1])) %>% rename(y.true = value)

training.index <- createDataPartition(y = y, times = 1, p = 0.8)
df <- df %>% mutate(which.data = ifelse(1:n() %in% training.index$Resample1, "training", "test"))

training <- df %>% filter(which.data == "training")
test <- df %>% filter(which.data == "test")
```


#Step 1 

To estimate a low-flexibility local linear model on the training data, we use the command npreg

```{r, include=FALSE, results=FALSE}
ll.fit.lowflex <- npreg(y ~ x, data = training, method = "ll", bws = 0.5)
summary(ll.fit.lowflex)
```


#Step 2

To estimate a high-flexibility local linear model on the training data, we use the command npreg

```{r, include=FALSE, results=FALSE}
ll.fit.highflex <- npreg(y ~ x, data = training, method = "ll", bws = 0.01)
summary(ll.fit.highflex)
``` 

#Step 3

Here, we plot the scatterplot of x-y along with the corresponding predictions (on the training data) using the ggplot command.
```{r, results= TRUE, echo= FALSE}
df <- df %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = df),
y.ll.highflex = predict(object = ll.fit.highflex, newdata = df))
training <- training %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex,
newdata = training), y.ll.highflex = predict(object = ll.fit.highflex, newdata = training))
training



ggplot(training) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.ll.lowflex), color = "red") + 
  geom_line(mapping = aes(x = x, y = y.ll.highflex), color = "blue")
```


#Step4

According to the previous graph, the more variable predictions are the ones generated by the high-flexibility local linear model.

The bias represents the error. So, the predictions made by the low-flexibility local linear model are the ones with the least bias.




#Step 5
```{r,echo= FALSE}
df <- df %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex, newdata = df),
y.ll.highflex = predict(object = ll.fit.highflex, newdata = df))
test <- test %>% mutate(y.ll.lowflex = predict(object = ll.fit.lowflex,
newdata = test), y.ll.highflex = predict(object = ll.fit.highflex, newdata = test))
test

ggplot(test) + geom_point(mapping = aes(x = x, y = y)) + 
  geom_line(mapping = aes(x = x, y = y.true)) + 
  geom_line(mapping = aes(x = x, y = y.ll.lowflex), color = "red") + 
  geom_line(mapping = aes(x = x, y = y.ll.highflex), color = "blue")
```

The more variable predictions are still the ones generated by the high-flexibility local linear model. But now, they are a little bit less changeable.

As for the predictions of the low-flexibility local linear model, their bias is smaller which means that predictions are more reliable.

#Step 6

```{r, results= FALSE}
bw <- seq(0.01, 0.5, by = 0.001)
```

#Step 7

```{r, results= FALSE}
llbw.fit <- lapply(X = bw, FUN = function(bw) {npreg(y ~ x, data = training, method = "ll", bws = bw)})
``` 

#Step 8

```{r, results= FALSE}
mse.training <- function(fit.model){
  predictions <- predict(object = fit.model, newdata = training)
  training %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}
mse.train.results <- unlist(lapply(X = llbw.fit, FUN = mse.training))
``` 

#Step 9

```{r, results= FALSE}
mse.test <- function(fit.model){
  predictions <- predict(object = fit.model, newdata = test)
  test %>% mutate(squared.error = (y - predictions)^2) %>% summarize(mse = mean(squared.error))
}
mse.test.results <- unlist(lapply(X = llbw.fit, FUN = mse.test))
```

#Step 10 

```{r, echo= FALSE}
mse.df <- tbl_df(data.frame(bandwidth = bw, mse.train = mse.train.results, mse.test = mse.test.results))
mse.df

ggplot(mse.df) + 
  geom_line(mapping = aes(x = bandwidth, y = mse.train), color = "blue") +
  geom_line(mapping = aes(x = bandwidth, y = mse.test), color = "orange")
```

Here, when the bandwidth increases the MSE on training data gradually increases.
When the same event happens, the MSE on test data first decreases. Then at the intersection point with the MSE on training data, we see that it changes course and also gradually increases. 



##Task 3B - Privacy Regulation compliance in France

#Step 1

``` {r}
library(stringr)
```

```{r}
cnil<- read.csv2(file="~/rprog/OpenCNIL_Organismes_avec_CIL_VD_20171115.csv")
departement<- read.csv(file="~/rprog/departement.csv")
```


#Step 2

```{r, echo= FALSE}
dept <- str_sub(cnil$Code_Postal, 1, 2)
dept2 <- data.frame(cnil, dept)
table <- table(dept2$dept)
table2 <- data.frame(table)[-(1:2),]
nicetable <- data.frame(table2)[-(98:109),]
names(nicetable)<- c("dept","freq")

tab <- merge(nicetable,departement,by.x="dept", by.y = "departmentCode")
tablefinale <- data.frame(tab$departmentName,tab$freq)
names(tablefinale)<- c("Department","Number of occurrences ")
tablefinale

```

